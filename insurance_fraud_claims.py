# -*- coding: utf-8 -*-
"""Insurance Fraud Claims.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PFo_fpVn1Bje_waxAVERoHY08dg_orJV
"""

# import data

import pandas as pd

df = pd.read_csv('insurance_fraud_claims.csv')
print(df.shape)
df.head()

# Different types of data

df.dtypes

# Descriptive statistic
df.describe()

# Missing value on _c39 - already NaN
df.isnull().sum().sort_values()

# Replacing ? with NaN

import numpy as np
df.replace("?", np.NaN)
df.head()

# Quick histogram of showing age groups

df['age'].hist(bins=72)

# Quick scatter plot to show correlation between age and months_as_customer

df.plot.scatter('age', 'months_as_customer');

# Scatter plot for capital gains

fig = px.scatter_3d(df, x='capital-gains', y='total_claim_amount', z='age',
                    color='incident_type')
fig.show()

# Scatter plot for capital loss

fig = px.scatter_3d(df, x='capital-loss', y='total_claim_amount', z='age',
                    color='incident_type')
fig.show()

# Baseline - stands at 0.75% of fraud not reported
# My target is 'fraud_reported' which is a type of binary data 
# A baseline is a method that uses simple summary statistics which is creates predictions for a dataset. 
# To measure accuracy and to compare back to once you further analyse your data and do more accuracy testing.

df['fraud_reported'].value_counts(normalize=True)

# We can see by this simple visualisation that there were not many frauds reported

df.fraud_reported.value_counts().plot.bar()

# Train Test

#The training set contains a known output and the model learns on this data
#test data is used to evaluate its accuracy

from sklearn.model_selection import train_test_split

train, test = train_test_split(df, train_size=0.80, test_size=0.20, random_state=2)

train.shape

test.shape

# Train Validate

train, val = train_test_split(train, train_size=0.80, test_size=0.20, random_state=42)

train.shape

val.shape

target = 'fraud_reported'
features = ['months_as_customer', 'incident_location' , 'total_claim_amount' , 'age' , 'capital-gains' , 'capital-loss' ]

X_train = train[features]
X_val = val[features]
X_test = test[features]

y_train = train[target]
y_val = val[target]
y_test = test[target]

# Imports 

!pip install category_encoders
import category_encoders as ce
import pandas as pd
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from sklearn.tree import DecisionTreeClassifier
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from scipy.stats import randint, uniform
from sklearn.model_selection import RandomizedSearchCV

# Test Accuracy 

X_test

X_train.head()

y_train.head()

# Regression problem

# Classification problem
# Evaluation metric - accuracy score

pipeline = make_pipeline(
    ce.OrdinalEncoder(), 
    SimpleImputer(strategy='median'), 
    RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
)

# Fit on train
pipeline.fit(X_train, y_train)
print('Test Accuracy', pipeline.score(X_test, y_test))

# This test accuracy is less than my baseline

